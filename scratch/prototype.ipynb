{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook prototypes an Optical Character Recognition (OCR) model using pytoch.\n",
    "\n",
    "- Train-test data: created using the trdg package.\n",
    "- Model: CNN-LSTM based architecture: <a href=\"https://arxiv.org/pdf/1507.05717.pdf\"> An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</a>, 2015, Shi et.al..\n",
    "- Loss function: Connectionist Temporal Classification (CTC): <a href=\"https://www.cs.toronto.edu/~graves/icml_2006.pdf\">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</a>, 2006, Alex Graves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "The trdg package is used to generate text images. \n",
    "The images are stored in the ./out folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 11:02:37.978763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-14 11:02:42.419705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|███████████████████████████████████████| 1000/1000 [00:26<00:00, 37.97it/s]\n"
     ]
    }
   ],
   "source": [
    "!trdg -c 1000 -w 5 -f 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peka/.venv/lib/python3.10/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# import pdb\n",
    "# import six\n",
    "# import random\n",
    "# import lmdb\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "# from itertools import chain\n",
    "import logging\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from torch.utils.data import random_split\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "# from src.utils.utils import AverageMeter, Eval, OCRLabelConverter\n",
    "# from src.utils.utils import EarlyStopping, gmkdir\n",
    "# from src.optim.optimizer import STLR\n",
    "# from src.utils.utils import gaussian\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each image to Grayscale -> tensor -> normalize.\n",
    "class SynthDataset(Dataset):\n",
    "    def __init__(self, opt):\n",
    "        super(SynthDataset, self).__init__()\n",
    "        self.path = os.path.join(opt.path, opt.imgdir)\n",
    "        self.images = os.listdir(self.path)\n",
    "        self.nSamples = len(self.images)\n",
    "        f = lambda x: os.path.join(self.path, x)\n",
    "        self.imagepaths = list(map(f, self.images))\n",
    "        transform_list =  [transforms.Grayscale(1),\n",
    "                            transforms.ToTensor(), \n",
    "                            transforms.Normalize((0.5,), (0.5,))]\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "        # self.collate_fn = SynthCollator()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "        imagepath = self.imagepaths[index]\n",
    "        imagefile = os.path.basename(imagepath)\n",
    "        img = Image.open(imagepath)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        item = {'img': img, 'idx':index}\n",
    "        item['label'] = imagefile.split('_')[0]\n",
    "        return item \n",
    "\n",
    "\n",
    "# Preparation for mini-batch gradient descent\n",
    "# Adjust the size of the images to be the same\n",
    "# The idea is to locate the image with the maximum width and adjust all the images to the same width.\n",
    "class SynthCollator(object):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "\n",
    "        # img_path = [item['img_path'] for item in batch]\n",
    "        width = [item['img'].shape[2] for item in batch]\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "        imgs = torch.ones([len(batch), batch[0]['img'].shape[0], batch[0]['img'].shape[1], max(width)], dtype=torch.float32)\n",
    "        for idx, item in enumerate(batch):\n",
    "            try:\n",
    "                imgs[idx, :, :, 0:item['img'].shape[2]] = item['img']\n",
    "            except:\n",
    "                print(imgs.shape)\n",
    "        item = {'img': imgs, 'idx':indexes}\n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label'] = labels\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "CNN-LSTM based architecture: Shi et.al., 2015. <a href=\"https://arxiv.org/pdf/1507.05717.pdf\"> An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</a>.\n",
    "\n",
    "- Feature extraction: 7-layered Convolution network with BatchNorm and ReLU\n",
    "- Sequence classifier: Two Bidirectional LSTM layers. This layer outputs the probability associated with each output class at each time step.\n",
    "\n",
    "\n",
    "The following code snippet is from the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "    def forward(self, input):\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, opt, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        assert opt['imgH'] % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = opt['nChannels'] if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential()\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(opt['nHidden']*2, opt['nHidden'], opt['nHidden']),\n",
    "            BidirectionalLSTM(opt['nHidden'], opt['nHidden'], opt['nClasses']))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        # rnn features\n",
    "        output = self.rnn(conv)\n",
    "        output = output.transpose(1,0) #Tbh to bth\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC Loss\n",
    "\n",
    "Connectionist Temporal Classification (CTC): <a href=\"https://www.cs.toronto.edu/~graves/icml_2006.pdf\">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</a>, 2006, Alex Graves.\n",
    "\n",
    "This layer takes the model's output (i.e. the LSTM layer's output) and computes a score with all possible alignments of the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCTCLoss(torch.nn.Module):\n",
    "    # T x B x H => Softmax on dimension 2\n",
    "    def __init__(self, dim=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "\n",
    "    def forward(self, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        EPS = 1e-7\n",
    "        loss = self.ctc_loss(logits, labels, prediction_sizes, target_sizes)\n",
    "        loss = self.sanitize(loss)\n",
    "        return self.debug(loss, logits, labels, prediction_sizes, target_sizes)\n",
    "    \n",
    "    def sanitize(self, loss):\n",
    "        EPS = 1e-7\n",
    "        if abs(loss.item() - float('inf')) < EPS:\n",
    "            return torch.zeros_like(loss)\n",
    "        if math.isnan(loss.item()):\n",
    "            return torch.zeros_like(loss)\n",
    "        return loss\n",
    "\n",
    "    def debug(self, loss, logits, labels,\n",
    "            prediction_sizes, target_sizes):\n",
    "        if math.isnan(loss.item()):\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"logits:\", logits)\n",
    "            print(\"labels:\", labels)\n",
    "            print(\"prediction_sizes:\", prediction_sizes)\n",
    "            print(\"target_sizes:\", target_sizes)\n",
    "            raise Exception(\"NaN loss obtained. But why?\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRTrainer(object):\n",
    "    def __init__(self, opt):\n",
    "        super(OCRTrainer, self).__init__()\n",
    "        self.data_train = opt['data_train']\n",
    "        self.data_val = opt['data_val']\n",
    "        self.model = opt['model']\n",
    "        self.criterion = opt['criterion']\n",
    "        self.optimizer = opt['optimizer']\n",
    "        self.schedule = opt['schedule']\n",
    "        self.converter = OCRLabelConverter(opt['alphabet'])\n",
    "        self.evaluator = Eval()\n",
    "        print('Scheduling is {}'.format(self.schedule))\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=opt['epochs'])\n",
    "        self.batch_size = opt['batch_size']\n",
    "        self.count = opt['epoch']\n",
    "        self.epochs = opt['epochs']\n",
    "        self.cuda = opt['cuda']\n",
    "        self.collate_fn = opt['collate_fn']\n",
    "        self.init_meters()\n",
    "\n",
    "    def init_meters(self):\n",
    "        self.avgTrainLoss = AverageMeter(\"Train loss\")\n",
    "        self.avgTrainCharAccuracy = AverageMeter(\"Train Character Accuracy\")\n",
    "        self.avgTrainWordAccuracy = AverageMeter(\"Train Word Accuracy\")\n",
    "        self.avgValLoss = AverageMeter(\"Validation loss\")\n",
    "        self.avgValCharAccuracy = AverageMeter(\"Validation Character Accuracy\")\n",
    "        self.avgValWordAccuracy = AverageMeter(\"Validation Word Accuracy\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits.transpose(1, 0)\n",
    "\n",
    "    def loss_fn(self, logits, targets, pred_sizes, target_sizes):\n",
    "        loss = self.criterion(logits, targets, pred_sizes, target_sizes)\n",
    "        return loss\n",
    "\n",
    "    def step(self):\n",
    "        self.max_grad_norm = 0.05\n",
    "        clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def schedule_lr(self):\n",
    "        if self.schedule:\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def _run_batch(self, batch, report_accuracy=False, validation=False):\n",
    "        input_, targets = batch['img'], batch['label']\n",
    "        targets, lengths = self.converter.encode(targets)\n",
    "        logits = self.forward(input_)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "        targets= targets.view(-1).contiguous()\n",
    "        loss = self.loss_fn(logits, targets, pred_sizes, lengths)\n",
    "        if report_accuracy:\n",
    "            probs, preds = logits.max(2)\n",
    "            preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "            sim_preds = self.converter.decode(preds.data, pred_sizes.data, raw=False)\n",
    "            ca = np.mean((list(map(self.evaluator.char_accuracy, list(zip(sim_preds, batch['label']))))))\n",
    "            wa = np.mean((list(map(self.evaluator.word_accuracy, list(zip(sim_preds, batch['label']))))))\n",
    "        return loss, ca, wa\n",
    "\n",
    "    def run_epoch(self, validation=False):\n",
    "        if not validation:\n",
    "            loader = self.train_dataloader()\n",
    "            pbar = tqdm(loader, desc='Epoch: [%d]/[%d] Training'%(self.count, \n",
    "                self.epochs), leave=True)\n",
    "            self.model.train()\n",
    "        else:\n",
    "            loader = self.val_dataloader()\n",
    "            pbar = tqdm(loader, desc='Validating', leave=True)\n",
    "            self.model.eval()\n",
    "        outputs = []\n",
    "        for batch_nb, batch in enumerate(pbar):\n",
    "            if not validation:\n",
    "                output = self.training_step(batch)\n",
    "            else:\n",
    "                output = self.validation_step(batch)\n",
    "            pbar.set_postfix(output)\n",
    "            outputs.append(output)\n",
    "        self.schedule_lr()\n",
    "        if not validation:\n",
    "            result = self.train_end(outputs)\n",
    "        else:\n",
    "            result = self.validation_end(outputs)\n",
    "        return result\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, ca, wa = self._run_batch(batch, report_accuracy=True)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.step()\n",
    "        output = OrderedDict({\n",
    "            'loss': abs(loss.item()),\n",
    "            'train_ca': ca.item(),\n",
    "            'train_wa': wa.item()\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss, ca, wa = self._run_batch(batch, report_accuracy=True, validation=True)\n",
    "        output = OrderedDict({\n",
    "            'val_loss': abs(loss.item()),\n",
    "            'val_ca': ca.item(),\n",
    "            'val_wa': wa.item()\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # logging.info('training data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_train,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn,\n",
    "                shuffle=True)\n",
    "        return loader\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        # logging.info('val data loader called')\n",
    "        loader = torch.utils.data.DataLoader(self.data_val,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.collate_fn)\n",
    "        return loader\n",
    "\n",
    "    def train_end(self, outputs):\n",
    "        for output in outputs:\n",
    "            self.avgTrainLoss.add(output['loss'])\n",
    "            self.avgTrainCharAccuracy.add(output['train_ca'])\n",
    "            self.avgTrainWordAccuracy.add(output['train_wa'])\n",
    "\n",
    "        train_loss_mean = abs(self.avgTrainLoss.compute())\n",
    "        train_ca_mean = self.avgTrainCharAccuracy.compute()\n",
    "        train_wa_mean = self.avgTrainWordAccuracy.compute()\n",
    "\n",
    "        result = {'train_loss': train_loss_mean, 'train_ca': train_ca_mean,\n",
    "        'train_wa': train_wa_mean}\n",
    "        # result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': train_loss_mean}\n",
    "        return result\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        for output in outputs:\n",
    "            self.avgValLoss.add(output['val_loss'])\n",
    "            self.avgValCharAccuracy.add(output['val_ca'])\n",
    "            self.avgValWordAccuracy.add(output['val_wa'])\n",
    "\n",
    "        val_loss_mean = abs(self.avgValLoss.compute())\n",
    "        val_ca_mean = self.avgValCharAccuracy.compute()\n",
    "        val_wa_mean = self.avgValWordAccuracy.compute()\n",
    "\n",
    "        result = {'val_loss': val_loss_mean, 'val_ca': val_ca_mean,\n",
    "        'val_wa': val_wa_mean}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    def __init__(self, model, optimizer, savepath=None, resume=False):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.savepath = os.path.join(savepath, 'best.ckpt')\n",
    "        self.cuda = torch.cuda.is_available() \n",
    "        self.cuda_count = torch.cuda.device_count()\n",
    "        if self.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        self.epoch = 0\n",
    "        if self.cuda_count > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.best_score = None\n",
    "        if resume and os.path.exists(self.savepath):\n",
    "            self.checkpoint = torch.load(self.savepath)\n",
    "            self.epoch = self.checkpoint['epoch']\n",
    "            self.best_score=self.checkpoint['best']\n",
    "            self.load()\n",
    "        else:\n",
    "            print('checkpoint does not exist')\n",
    "\n",
    "    def fit(self, opt):\n",
    "        opt['cuda'] = self.cuda\n",
    "        opt['model'] = self.model\n",
    "        opt['optimizer'] = self.optimizer\n",
    "        logging.basicConfig(filename=\"%s/%s.csv\" %(opt['log_dir'], opt['name']), level=logging.INFO)\n",
    "        self.saver = EarlyStopping(self.savepath, patience=15, verbose=True, best_score=self.best_score)\n",
    "        opt['epoch'] = self.epoch\n",
    "        trainer = OCRTrainer(opt)\n",
    "        \n",
    "        for epoch in range(opt['epoch'], opt['epochs']):\n",
    "            train_result = trainer.run_epoch()\n",
    "            val_result = trainer.run_epoch(validation=True)\n",
    "            trainer.count = epoch\n",
    "            info = '%d, %.6f, %.6f, %.6f, %.6f, %.6f, %.6f'%(epoch, train_result['train_loss'], \n",
    "                val_result['val_loss'], train_result['train_ca'],  val_result['val_ca'],\n",
    "                train_result['train_wa'], val_result['val_wa'])\n",
    "            logging.info(info)\n",
    "            self.val_loss = val_result['val_loss']\n",
    "            print(self.val_loss)\n",
    "            if self.savepath:\n",
    "                self.save(epoch)\n",
    "            if self.saver.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def load(self):\n",
    "        print('Loading checkpoint at {} trained for {} epochs'.format(self.savepath, self.checkpoint['epoch']))\n",
    "        self.model.load_state_dict(self.checkpoint['state_dict'])\n",
    "        if 'opt_state_dict' in self.checkpoint.keys():\n",
    "            print('Loading optimizer')\n",
    "            self.optimizer.load_state_dict(self.checkpoint['opt_state_dict'])\n",
    "\n",
    "    def save(self, epoch):\n",
    "        self.saver(self.val_loss, epoch, self.model, self.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m alphabet \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mOnly thewigsofrcvdampbkuq.$A-210xT5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMDL,RYHJ\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISPWENj&BC93VGFKz();#:!7U64Q8?+*ZX/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \n\u001b[1;32m     19\u001b[0m }\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mSynthDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollate_fn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m SynthCollator()\n\u001b[1;32m     23\u001b[0m train_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mSynthDataset.__init__\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, opt):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(SynthDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m, opt\u001b[38;5;241m.\u001b[39mimgdir)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "alphabet = \"\"\"Only thewigsofrcvdampbkuq.$A-210xT5'MDL,RYHJ\"ISPWENj&BC93VGFKz();#:!7U64Q8?+*ZX/%\"\"\"\n",
    "args = {\n",
    "    'name':'exp1',\n",
    "    'path':'data',\n",
    "    'imgdir': 'train',\n",
    "    'imgH':32,\n",
    "    'nChannels':1,\n",
    "    'nHidden':256,\n",
    "    'nClasses':len(alphabet),\n",
    "    'lr':0.001,\n",
    "    'epochs':4,\n",
    "    'batch_size':32,\n",
    "    'save_dir':'checkpoints',\n",
    "    'log_dir':'logs',\n",
    "    'resume':False,\n",
    "    'cuda':False,\n",
    "    'schedule':False\n",
    "    \n",
    "}\n",
    "\n",
    "data = SynthDataset(args)\n",
    "args['collate_fn'] = SynthCollator()\n",
    "train_split = int(0.8*len(data))\n",
    "val_split = len(data) - train_split\n",
    "args['data_train'], args['data_val'] = random_split(data, (train_split, val_split))\n",
    "print('Traininig Data Size:{}\\nVal Data Size:{}'.format(\n",
    "    len(args['data_train']), len(args['data_val'])))\n",
    "args['alphabet'] = alphabet\n",
    "model = CRNN(args)\n",
    "args['criterion'] = CustomCTCLoss()\n",
    "savepath = os.path.join(args['save_dir'], args['name'])\n",
    "gmkdir(savepath)\n",
    "gmkdir(args['log_dir'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "learner = Learner(model, optimizer, savepath=savepath, resume=args['resume'])\n",
    "learner.fit(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_accuracy(args):\n",
    "    loader = torch.utils.data.DataLoader(args['data'],\n",
    "                batch_size=args['batch_size'],\n",
    "                collate_fn=args['collate_fn'])\n",
    "    model = args['model']\n",
    "    model.eval()\n",
    "    converter = OCRLabelConverter(args['alphabet'])\n",
    "    evaluator = Eval()\n",
    "    labels, predictions, images = [], [], []\n",
    "    for iteration, batch in enumerate(tqdm(loader)):\n",
    "        input_, targets = batch['img'].to(device), batch['label']\n",
    "        images.extend(input_.squeeze().detach())\n",
    "        labels.extend(targets)\n",
    "        targets, lengths = converter.encode(targets)\n",
    "        logits = model(input_).transpose(1, 0)\n",
    "        logits = torch.nn.functional.log_softmax(logits, 2)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "        probs, pos = logits.max(2)\n",
    "        pos = pos.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = converter.decode(pos.data, pred_sizes.data, raw=False)\n",
    "        predictions.extend(sim_preds)\n",
    "        \n",
    "#     make_grid(images[:10], nrow=2)\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    columns = 4\n",
    "    rows = 5\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img = images[i]\n",
    "        img = (img - img.min())/(img.max() - img.min())\n",
    "        img = np.array(img * 255.0, dtype=np.uint8)\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.title(predictions[i])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "    ca = np.mean((list(map(evaluator.char_accuracy, list(zip(predictions, labels))))))\n",
    "    wa = np.mean((list(map(evaluator.word_accuracy_line, list(zip(predictions, labels))))))\n",
    "    return ca, wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgdir\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mSynthDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m resume_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_dir\u001b[39m\u001b[38;5;124m'\u001b[39m], args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resume_file):\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mSynthDataset.__init__\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, opt):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(SynthDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m, opt\u001b[38;5;241m.\u001b[39mimgdir)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnSamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "args['imgdir'] = 'test'\n",
    "args['data'] = SynthDataset(args)\n",
    "resume_file = os.path.join(args['save_dir'], args['name'], 'best.ckpt')\n",
    "if os.path.isfile(resume_file):\n",
    "    print('Loading model %s'%resume_file)\n",
    "    checkpoint = torch.load(resume_file)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    args['model'] = model\n",
    "    ca, wa = get_accuracy(args)\n",
    "    print(\"Character Accuracy: %.2f\\nWord Accuracy: %.2f\"%(ca, wa))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(save_file))\n",
    "    print('Exiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
